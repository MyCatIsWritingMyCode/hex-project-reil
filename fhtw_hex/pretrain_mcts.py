import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from tqdm import trange
import numpy as np
import argparse
import os

from hex_engine import hexPosition
from networks import ResNet, ActorCritic
from a2c_agent import select_action

class ExpertDataset(Dataset):
    """A dataset for storing expert moves generated by a teacher agent."""
    def __init__(self, states, policies, values):
        self.states = states
        self.policies = policies
        self.values = values

    def __len__(self):
        return len(self.states)

    def __getitem__(self, idx):
        return self.states[idx], self.policies[idx], self.values[idx]

def generate_expert_data(args, teacher_agent, device):
    """
    Generates a dataset of expert moves by having the teacher agent play against itself.
    """
    print("--- Generating Expert Data ---")
    all_states, all_policies, all_values = [], [], []
    
    for _ in trange(args.num_games, desc="Generating Games"):
        env = hexPosition(args.board_size)
        game_states, game_policies = [], []
        
        while env.winner == 0:
            # Get the canonical board state from the perspective of the current player
            board_state = np.array(env.board, dtype=np.float32)
            if env.player == -1:
                canonical_board = board_state * -1
            else:
                canonical_board = board_state
            
            # Get the expert policy from the teacher agent by calling it correctly
            with torch.no_grad():
                board_tensor = torch.from_numpy(canonical_board).unsqueeze(0).unsqueeze(0).to(device)
                policy_probs, _ = teacher_agent(board_tensor)
            
            valid_moves = env.get_action_space()
            action_coords, _ = select_action(policy_probs.cpu(), valid_moves, args.board_size)
            policy_numpy = policy_probs.squeeze().cpu().numpy()

            # Store the state and the policy
            game_states.append(canonical_board)
            game_policies.append(policy_numpy)
            
            # Make a move in the environment
            env.move(action_coords)
            
        # Determine the winner and assign values
        winner = env.winner
        for i, _ in enumerate(game_states):
            # The value is from the perspective of the player who made the move at that state.
            player_at_turn = 1 if (i % 2) == 0 else -1
            value = winner * player_at_turn
            all_values.append(value)

        all_states.extend(game_states)
        all_policies.extend(game_policies)

    print(f"Generated {len(all_states)} expert moves from {args.num_games} games.")
    
    # Convert to tensors
    states_tensor = torch.FloatTensor(np.array(all_states))
    policies_tensor = torch.FloatTensor(np.array(all_policies))
    values_tensor = torch.FloatTensor(np.array(all_values)).unsqueeze(1)
    
    return ExpertDataset(states_tensor, policies_tensor, values_tensor)

def pretrain_network(args, teacher_agent, mcts_network, device):
    """
    Pre-trains the MCTS network on data generated by the teacher agent.
    """
    dataset = generate_expert_data(args, teacher_agent, device)
    dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True)
    
    optimizer = optim.Adam(mcts_network.parameters(), lr=args.learning_rate)
    policy_loss_fn = nn.CrossEntropyLoss()
    value_loss_fn = nn.MSELoss()

    print("\n--- Pre-training MCTS Network ---")
    mcts_network.train()
    for epoch in range(args.epochs):
        total_policy_loss = 0
        total_value_loss = 0
        for states, target_policies, target_values in dataloader:
            states = states.to(device)
            target_policies = target_policies.to(device)
            target_values = target_values.to(device)

            # Add channel dimension to states
            states = states.unsqueeze(1)

            # Forward pass
            pred_policies, pred_values = mcts_network(states)
            
            # Calculate loss
            policy_loss = policy_loss_fn(pred_policies, target_policies)
            value_loss = value_loss_fn(pred_values, target_values)
            loss = policy_loss + value_loss
            
            # Backward pass and optimization
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            total_policy_loss += policy_loss.item()
            total_value_loss += value_loss.item()
            
        avg_policy_loss = total_policy_loss / len(dataloader)
        avg_value_loss = total_value_loss / len(dataloader)
        print(f"Epoch {epoch+1}/{args.epochs} | Policy Loss: {avg_policy_loss:.4f} | Value Loss: {avg_value_loss:.4f}")
        
    # Save the pre-trained model
    output_dir = os.path.dirname(args.output_model_path)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir, exist_ok=True)
    
    torch.save(mcts_network.state_dict(), args.output_model_path)
    print(f"\nPre-trained MCTS model saved to {args.output_model_path}")


def run_mcts_pretraining(args):
    # Setup device
    device = torch.device("mps" if torch.backends.mps.is_available() and args.environment == 'apple' else "cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # Load teacher model
    print(f"Loading teacher agent from {args.teacher_model_path}...")
    teacher_agent = ActorCritic(args.board_size, args.board_size**2).to(device)
    teacher_agent.load_state_dict(torch.load(args.teacher_model_path, map_location=device))
    teacher_agent.eval()

    # Initialize MCTS network (we'll use ResNet as it's more powerful)
    print("Initializing new MCTS network (ResNet)...")
    mcts_network = ResNet(args.board_size, args.board_size**2).to(device)

    # Run pre-training
    pretrain_network(args, teacher_agent, mcts_network, device)
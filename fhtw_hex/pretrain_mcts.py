import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
from tqdm import tqdm, trange
import numpy as np
import argparse
import os

from hex_engine import hexPosition
from networks import ResNet, ActorCritic
from a2c_agent import select_action

class ExpertDataset(Dataset):
    """A dataset for storing expert moves generated by a teacher agent."""
    def __init__(self, states, policies, values):
        self.states = states
        self.policies = policies
        self.values = values

    def __len__(self):
        return len(self.states)

    def __getitem__(self, idx):
        return self.states[idx], self.policies[idx], self.values[idx]

def generate_expert_data(args, teacher_agent, device):
    """
    Generates a dataset of expert moves by having the teacher agent play against itself.
    """
    print("--- Generating Expert Data ---")
    all_states, all_policies, all_values = [], [], []
    
    for _ in trange(args.num_games, desc="Generating Games"):
        env = hexPosition(args.board_size)
        game_states, game_policies = [], []
        
        while env.winner == 0:
            # Get the canonical board state from the perspective of the current player
            board_state = np.array(env.board, dtype=np.float32)
            if env.player == -1:
                canonical_board = board_state * -1
            else:
                canonical_board = board_state
            
            # Get the expert policy from the teacher agent by calling it correctly
            with torch.no_grad():
                board_tensor = torch.from_numpy(canonical_board).unsqueeze(0).unsqueeze(0).to(device)
                policy_probs, _ = teacher_agent(board_tensor)
            
            valid_moves = env.get_action_space()
            action_coords, _ = select_action(policy_probs.cpu(), valid_moves, args.board_size)
            policy_numpy = policy_probs.squeeze().cpu().numpy()

            # Store the state and the policy
            game_states.append(canonical_board)
            game_policies.append(policy_numpy)
            
            # Make a move in the environment
            env.move(action_coords)
            
        # Determine the winner and assign values
        winner = env.winner
        for i, _ in enumerate(game_states):
            # The value is from the perspective of the player who made the move at that state.
            player_at_turn = 1 if (i % 2) == 0 else -1
            value = winner * player_at_turn
            all_values.append(value)

        all_states.extend(game_states)
        all_policies.extend(game_policies)

    print(f"Generated {len(all_states)} expert moves from {args.num_games} games.")
    
    # Convert to tensors
    states_tensor = torch.FloatTensor(np.array(all_states))
    policies_tensor = torch.FloatTensor(np.array(all_policies))
    values_tensor = torch.FloatTensor(np.array(all_values)).unsqueeze(1)
    
    return ExpertDataset(states_tensor, policies_tensor, values_tensor)

def pretrain_network(args, teacher_agent, mcts_network, device):
    """
    Pre-trains the MCTS network on data generated by the teacher agent.
    """
    full_dataset = generate_expert_data(args, teacher_agent, device)
    
    # Split dataset into training and validation sets
    train_size = int(0.8 * len(full_dataset))
    val_size = len(full_dataset) - train_size
    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])
    
    train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)
    val_dataloader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False)
    
    optimizer = optim.Adam(mcts_network.parameters(), lr=args.learning_rate)
    policy_loss_fn = nn.CrossEntropyLoss()
    value_loss_fn = nn.MSELoss()

    print("\n--- Pre-training MCTS Network ---")
    
    best_val_loss = float('inf')
    patience = 10  # Number of epochs to wait for improvement before stopping
    patience_counter = 0

    mcts_network.train()
    epoch_iterator = trange(args.epochs, desc="Epoch")
    for epoch in epoch_iterator:
        total_train_policy_loss = 0
        total_train_value_loss = 0
        
        # Training loop with progress bar
        mcts_network.train()
        train_loop = tqdm(train_dataloader, desc=f"Training Epoch {epoch+1}/{args.epochs}", leave=False)
        for states, target_policies, target_values in train_loop:
            states, target_policies, target_values = states.to(device), target_policies.to(device), target_values.to(device)
            states = states.unsqueeze(1)

            pred_policies, pred_values = mcts_network(states)
            
            policy_loss = policy_loss_fn(pred_policies, target_policies)
            value_loss = value_loss_fn(pred_values, target_values)
            loss = policy_loss + value_loss
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            total_train_policy_loss += policy_loss.item()
            total_train_value_loss += value_loss.item()
            train_loop.set_postfix(policy_loss=policy_loss.item(), value_loss=value_loss.item())
            
        avg_train_policy_loss = total_train_policy_loss / len(train_dataloader)
        avg_train_value_loss = total_train_value_loss / len(train_dataloader)

        # Validation loop
        mcts_network.eval()
        total_val_loss = 0
        with torch.no_grad():
            for states, target_policies, target_values in val_dataloader:
                states, target_policies, target_values = states.to(device), target_policies.to(device), target_values.to(device)
                states = states.unsqueeze(1)
                
                pred_policies, pred_values = mcts_network(states)
                
                policy_loss = policy_loss_fn(pred_policies, target_policies)
                value_loss = value_loss_fn(pred_values, target_values)
                total_val_loss += (policy_loss + value_loss).item()

        avg_val_loss = total_val_loss / len(val_dataloader)
        epoch_iterator.set_description(f"Epoch | Train P-Loss: {avg_train_policy_loss:.4f}, V-Loss: {avg_train_value_loss:.4f} | Val Loss: {avg_val_loss:.4f}")

        # Early stopping check
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            patience_counter = 0
            # Optionally save the best model here
            # torch.save(mcts_network.state_dict(), "best_model.pth")
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f"\n--- Early stopping triggered after {epoch+1} epochs ---")
                break
        
    # Save the final pre-trained model
    output_dir = os.path.dirname(args.output_model_path)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir, exist_ok=True)
    
    torch.save(mcts_network.state_dict(), args.output_model_path)
    print(f"\nPre-trained MCTS model saved to {args.output_model_path}")


def run_mcts_pretraining(args):
    # Setup device
    device = torch.device("mps" if torch.backends.mps.is_available() and args.environment == 'apple' else "cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # Load teacher model
    print(f"Loading teacher agent from {args.teacher_model_path}...")
    teacher_agent = ActorCritic(args.board_size, args.board_size**2).to(device)
    teacher_agent.load_state_dict(torch.load(args.teacher_model_path, map_location=device))
    teacher_agent.eval()

    # Initialize MCTS network (we'll use ResNet as it's more powerful)
    print("Initializing new MCTS network (ResNet)...")
    mcts_network = ResNet(args.board_size, args.board_size**2).to(device)

    # Run pre-training
    pretrain_network(args, teacher_agent, mcts_network, device)